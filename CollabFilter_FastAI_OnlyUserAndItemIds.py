# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18RzLMIKaufx75tE69L2yRrXbR4zX3i2-
"""

!git clone https://github.com/SonaliDasgupta/goodbooks-10k

import pandas as pd

!pip install fastai pytorch

ratings_df = pd.read_csv('goodbooks-10k/ratings.csv')

from fastai.collab import CollabDataBunch
from fastai.collab import collab_learner

import torch.optim as optim
from fastai.metrics import exp_rmspe
data = CollabDataBunch.from_df(ratings_df) #no test dataset here
wd = 1e-3
m = collab_learner(data, n_factors = 50, y_range = (1,5), metrics = [exp_rmspe])
#m.opt_fn = optim.Adam(params = m.parameters, lr = 0.5)
#choosing 50 factors as of now, might try with half the size of dataset later
#ratings go from 1 to 5 hence y_range

from fastai.train import lr_find
lr_find(m)
m.recorder.plot_metrics

m.recorder.plot()

m.opt = optim.Adam(params = m.model.parameters(), lr = 0.03)
m.opt.mom = 0.9
m.fit(3, lr = 0.03, wd = 1e-5) #PULL CODE IN SPYDER  AND SEE OPTIM WRAPPER USAGE , TRY WITH ADAM LATER AND ALSO WEIGHT DECAY AND MOMENTUM
#TRY WITH BOTH ADAM AND SGD

#with SGD optimization
m1 = collab_learner(data, n_factors = 50, y_range = (1,5), metrics = [exp_rmspe])
m1.opt = optim.SGD(params = m1.model.parameters(), lr = 0.5, momentum = 0.9)
m1.opt.mom = 0.9
m1.fit(3, lr = 0.03)

#SGD performs way better
#TRY ONCE WITH SAME WEiGHT DECAY

#1.  interpret embeddings
#2. Create columnar model using the metatdata, and compare the 2 models on a metric
#3 scrape the web data from good reads website

m1.model.i_weight #analyzing movie categories
#from sklearn.decomposition import PCA

#perform inner join with books
books_df = pd.read_csv('goodbooks-10k/books.csv')

books_df.columns

ratings_df.columns

ratings_book_df = ratings_df.merge(books_df, how = 'left', on = 'book_id')

cols = [col for col in ratings_book_df.columns if col not in ratings_df.columns and col not in ['book_id','title', 'authors', 'language_code']]
ratings_book_df.drop(cols, axis= 1, inplace = True)

count_ratings = ratings_book_df.groupby('book_id')['rating'].count()

count_ratings

top300books = count_ratings.sort_values(ascending = False).index.values[:300]
top300books.dtype

books_df['book_id'].dtype

bias_books = m1.bias([str(book) for book in top300books])
bias_books.shape

book_names = {}
for book in top300books:
  book_names[book] = books_df.iloc[book, 10]

names_with_bias = pd.DataFrame({'bookName': book_names.values(), 'bias': bias_books}, columns=['bookName', 'bias'])
books_avg_bias = names_with_bias.groupby(['bookName']).agg({'bias': 'sum'})

books_avg_bias

data.classes